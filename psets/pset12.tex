\documentclass[../main.tex]{subfiles}
\begin{document}
\section*{Problem Set 12}
    Name: Fangyuan Lin, UC Berkeley, Class of 2024

\subsection*{10.6 Differential Entropy of a linear transformation of a random vector}
Let $A$ be $n\times n$ non-singular matrix. Then \[
h(A\vec X) = h(\vec X)+ \log |\det A|
\]
\begin{proof}
    Let $\vec Y=A\vec X$ be a linear transformation of the random vector $\vec X$. Then \[
    f_{\vec Y}(\vec y) = \frac{1}{|\det A|}f_{\vec X}(A^{-1}\vec y),
    \] which is a classical result.
    Then \begin{align*}
        h(\vec X) &= -\int_{\S_{\vec X}}f_{\vec X}(\vec x)\log f_{\vec X}(\vec x)\dd x\quad \text{by definition}\\
        &= - \int_{\S_{\vec Y}}\frac{1}{|\det(A)|}f_{\vec X}(A^{-1}\vec y)\log f_{\vec X}(A^{-1}\vec y)\dd \vec y \quad \text{change of variable}\\
        &= - \int_{\S_{\vec Y}}\frac{1}{|\det(A)|}f_{\vec X}(A^{-1}\vec y)(\log \frac{1}{|\det(A)|}f_{\vec X}(A^{-1}\vec y) + \log |\det A|)\dd \vec y\\
        &= h(A\vec X) - \log |\det A|
    \end{align*}
\end{proof}
\subsection*{10.7 Differential Entropy is not Self-Information!}
\begin{proof}
    Let $X$ be a continuous random variable with continuous pdf $f$. We know that the differential mutual information is the limit of the discrete mutual information of the quantizations of the variables as resolution $\Delta \to 0$. Consider \begin{align*}
        \I(\hat X_\Delta, \hat X_\Delta) &= H(\hat X_\Delta) \\
        &= -\sum_i \Pr\{\hat X_\Delta = i\}\log \Pr\{\hat X_\Delta = i\} \\
        &\approx -\sum_i f(x_i)\Delta \log f(x_i)\Delta\\
        &= -\sum_i f(x_i)\Delta \log f(x_i) - \log \Delta\sum_if(x_i)\Delta\\
        &\to h(X)-\log \Delta \quad \text{as $\Delta\to 0$}
    \end{align*}
    Therefore the self-information always goes to $\infty$ as the resolution $\Delta$ tends to $0$, meaning the mutual information can be equal to the differential entropy $h(X)$ if $h(X)$ is finite.
\end{proof}
\begin{remark}
    We derived a strong result that self-information is always infinite for continuous random variables.
\end{remark}
\subsection*{10.8 Distributions that maximize differential entropy under constraints.}
Identify the set of constraints for the distributions for which they maximizes the differential entropy.
\begin{enumerate}
    \item The exponential distribution: $f(x)=\lambda e^{-\lambda x}$. 
    \begin{proof}
        Recall that \[
        f^*(x) = e^{-\lambda_0 - \sum_{i=1}^m \lambda_i r_i(\vec x)}.
        \]
        Here $r_1(x)=x$. So our constraint is that $\int_{x\geq 0} x f(x) = \frac{1}{\lambda}$.
    \end{proof}
    \item The Laplace distribution with mean zero: $f(x)=\frac{1}{2}\lambda e^{-\lambda|x|}$.
    \begin{proof}
        Here $r_1(x)=|x|$. So our constraint is that the expectation of the absolute value of our zero-mean Laplace distribution is $\frac{1}{\lambda}$, i.e. \[
        \int |x| f(x) = \frac{1}{\lambda}.
        \]
    \end{proof}
\end{enumerate}
\subsection*{10.9 Upper bound on differential entropy by the mean}
Let $\mu$ be the mean of a continuous random variable $X$ defined on $\bR^+$. Obtain an upper bound on $h(X)$ in terms of $\mu$.
\begin{proof}

    Claim: an upper bound is $\ln \mu + 1$, since we can use result from part b of last question.
    \begin{enumerate}
        \item Let $Y$ follow exponential distribution with pdf \[
        f(y) = \lambda e^{-\lambda y}, y\geq 0
        \]
        \item Then $\bE Y = \frac{1}{\lambda}$ and we set $\lambda = \frac{1}{\mu}$. 
        \item Then \begin{align*}
            h(Y) &= -\int_0^\infty f(y)\ln f(y) \dd y\\
            &=-\int_0^\infty f(y)(\ln \lambda - \lambda y \dd y\\
            &= -\ln \lambda + \lambda \frac{1}{\lambda}\\
            &= -\ln \mu + 1
        \end{align*}
    \end{enumerate}
\end{proof}
\subsection*{10.12 Hadamard's inequality}
Show that for a positive semidefinite matrix $K$, $\det K\leq \prod_{i=1}^nK_{ii}$, with equality if and only if $K$ is diagonal. 
\newline
Hint: Consider the differential entropy of a multivariate Gaussian distribution.
\begin{proof}
    \begin{enumerate}
        \item If $K$ is diagonal or triangular, then equality is immediate. 
        \item Let $\vec X\sim \N(\vec 0, K)$. Then \[
        h(\vec X) =\frac{1}{2}\log(2\pi e \det K).
        \] and \[
        h(X_i) = \frac{1}{2}\log(2\pi e\det K_{ii})
        \]
        \item By the independence bound, 
        \begin{align*}
            h(\vec X) &\leq \sum_i h(X_i)\\
            \frac{1}{2}\log(2\pi e \det K) &\leq \sum_i \frac{1}{2}\log(2\pi e\det K_{ii}) \\
            \det K &\leq \prod_i \det K{ii}
        \end{align*}
        \item If we have equality, then we have uncorrelation which implies independence for multivariate Gaussian distribution, meaning we have tightness in the independence bound.
    \end{enumerate}
\end{proof}
\subsection*{10.13 Determinant of covariance is upper bounded by determinant of correlation}
Let $K_{\vec X}$ and $\hat K_{\vec X}$ be the covariance matrix and the correlation matrix of a random vector $\vec X$ respectively. Show that \[
\det K_{\vec X} \leq \det \hat K_{\vec X}.
\]
Note that this is a generalization of the fact that $var X \leq \bE X^2$.
Hint: Consider a multivariate Gaussian distribution with zero mean and the same correlation matrix.
\begin{proof}
    \begin{enumerate}
        \item We can use the fact that given a correlation matrix, the zero mean multivariate Gaussian maximizes the differential entropy.
        \item Let $\vec X \sim \N(\vec \mu, K)$ and $\vec{\hat X}\sim \N(\vec 0, \hat K)$.
        \item $\vec{\hat X}$ is zero mean, so the $\vec X$ and $\vec{\hat X}$ have the same correlation matrix. 
        \item Then \[
        \frac{1}{2}\log(2\pi e\det K_X) \leq \frac{1}{2}\log(2\pi e\det \hat K_X)
        \]
        \item $\det K_X \leq \det \hat K_X$
    \end{enumerate}
\end{proof}
\end{document}