\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{The I-Measure}
Information diagram:
Joint entropy is like the union. Mutual information is like the intersection. Conditional entropy of $X$ is like the entropy exclusive to $X$.\\
Why does this work? The answer lies in I-Measure. 
\section{The Second Law of Thermodynamics}
Second law: The entropy of an isolated system is non-decreasing. \\
In statistical thermodynamics, entropy is defined as the log of the number of microstates in the system: corresponding to information entropy if the states are equally likely.\\
We model an isolated system as a Markov chain with transitions governed by physical laws. There are 4 interpretations:
\begin{itemize}
    \item Relative entropy/KL divergence $D(\mu_n||\mu'_n)$ decreases with $n$, where $\mu_n$ and $\mu'_n$ are 2 probability distributions on the state space of a Markov chain at time $n$. Like Cauchy sequence lol.
    Proof uses chain rule of KL divergence, splitted in 2 different ways, and a key observation is that the transition probabilities are the same because we are in the same system.
    \item Relative entropy between a distribution on states at time $n$ and a stationary distribution decreases with $n$. This implies any state distribution gets closer and closer to the stationary distribution. This is a special case of the above.
    \item Entropy increases if the stationary distribution is uniform: because any distribution of states tend to the stationary distribution, and entropy is maximized by uniform distribution.
    \item $H(X_n|X_1)$ increases with $n$ for stationary process. Note by stationarity, marginal entropy $H(X_n)$ is constant.
    \begin{align*}
        &\H(X_n|X_1)\\
        &\geq H(X_n|X_1,X_2\\
        &= H(X_n|X_2) \hspace{5mm} \text{by Markovianity, and entropy is quantity for distribution}\\
        &=H(X_{n-1}|X_1) \hspace{5mm} \text{by stationarity}.
    \end{align*}
    We can also use the data processing inequality.
    \item Shuffles increase entropy: $H(TX)\geq H(X)$ where $T$ is a random permutation of $\X$.
\end{itemize}
\section{The Asymptotic Equipartiton Property AEP}
In information theory, the AEP is analogous to the law of large numbers.
\begin{bbox}{AEP}
    If $X_1,X_2,\dots$ are iid $\sim p(x)$, then \begin{equation*}
        -\frac{1}{n}\log p(X_1,\dots,X_n)\to H(X)
    \end{equation*} in measure.
    \begin{proof}
        Note functions of iid RVs are also iid RVs (in our case, the function refers to a probability measure), so by weak law of large numbers,\begin{align*}
            -\frac{1}{n}\log p(X_1,...,X_n) = -\frac{1}{n}\sum_i\log p(X_i) \hspace{5mm}\text{by independence}
            \\
            \to -E\log p(X) \text{in probabiilty}\\
            =H(X)
        \end{align*}
    \end{proof}
\end{bbox}
\section*{Preliminaries: The I-Measure for two random variables}
Substitution of Symbols:\begin{itemize}
    \item $H/I \iff \mu*$
    \item $, \iff \cup$
    \item $; \iff \cap$
    \item $| \iff -$ (set subtraction, i.e., intersection with complement)
    \item This is exactly how we define the signed measure $\mu^*$:
    \item $\mu^*(\hat{X_1}-\hat{X_2}):=H(X_1|X_2)$
    \item $\mu^*(\hat{X_2}-\hat{X_1})=H(X_2|X_1)$
    \item $\mu^*(\hat{X_1}\cap \hat{X_2})=\I(X_1;X_2)$
\end{itemize}
$\mu*$ is some measure (set-additive function, satisfy axioms for measure)\\
Inclusion-Exclusion formulation in set theory: \begin{equation*}
    \mu^*(X_1\cup X_2)=\mu^*(X_1)+\mu^*(X_2)-\mu^*(X_1\cap X_2)
\end{equation*} corresponding to $H(X_1,X_2)=H(X_1)+H(X_2)-I(X_1;X_2)$.
\begin{gbox}{Field}
    The field $F_n$ generated by sets $X_1,...,X_n$ is the collection os sets which can be obtained by any sequence of usual set operations (union, interaction, complement, and difference) on $X_1,...,X_n$.\\
    The atoms of $F_n$ are sets of the form $\cap_{i=1}^n Y_i$ where $Y_i$ is either $X_i$ or its complement.
\end{gbox}
The condition is stronger than necessary.\\
Examples: Field generated by 2 sets: 4 atoms.\\
A measure is completely specified by its values on the atoms.

\section*{Construction of the I-Measure $\mu^*$}
Let $\Tilde{X}$ be a set corresponding to a r.v. X.\\
Fix $n$ and let $N_n = \{1,\dots,n\}$.\\
Let the universal set be $\omega=\cup_{i\in N_n}\Tilde{X}_i$. \\
The atom $A_0=\cap_{i\in N_n}\Tilde{X}_i^C=\empty$ is called the empty atom of $F_n$.\\
Let $A$ denote the set of all other atoms of $F_n$, called non-empty atoms. $|A|=2^n-1$.\\
A measure $\mu$ on $F_n$ is completely specified by its values on the atoms of $A$.\\
Let $X_G=(X_i,i\in G)$. $\Tilde{X}_G=\cup_{i\in G} \Tilde{X_i}$.
\begin{bbox}{3.6}
    Let $\mathcal{B}=\{\Tilde{X}_G:\text{G is a nonempty subset of $N_n$}\}$. Then a measure $\mu$ on $F_n$ is completely specified by $\{\mu(B),B\in\mathcal{B}\}$, which can be any set of real numbers.
    \begin{remark}
        We saw that $\mu$ is determined by its values on $A$, the atoms. This theorem says $\mu$ can also be determined by its values on the unions.
    \end{remark}
    \begin{itemize}
        \item We can get that $\mu(A_1\cap A_2)=\mu(A_1)+\mu(A_2)-\mu(A_1\cup A_2)$, inclusion-exlusion formula (true for any set-additive measure).
    \end{itemize}
    \begin{proof}
        For any nonempty atom $A\in\mathcal{A}$, \begin{equation*}
            A=\Cap_{i=1}^nY_i
        \end{equation*} where $Y_i$ is either $\Tilde{X}_i$ pr $\Tilde{X}_i^C$. Then there exists at least one $i$ such that $Y_i=\Tilde{X}_i$; otherwise, $A$ would be equal to the empty atom.\\
        Next, we split the intersection into 2 parts...\\
        Eventually $\mu(B)$ is related to $\mu(A)$ through an invertible linear transformation.
    \end{proof}
\end{bbox}

\subsection{One step away from construction of the I-measure \texorpdfstring{$\mu^*$}{mu*}}

\begin{bbox}{3.7 3.8 Two Lemmas}
    \begin{itemize}
        \item \begin{equation*}
            \mu(A\cap B-C)= \mu(A\cup C)+\mu(B\cup C)-\mu(A\cup B\cup C)-\mu(C).
        \end{equation*}
        \item \begin{equation*}
            I(X;Y|Z)=H(X,Z) + H(X, Y) - H(X,Y,Z)-H(Z)
        \end{equation*}
    \end{itemize}
    \begin{proof}
        \begin{align*}
            \mu(A\cap B-C) &= \mu(A-c)+(B-C)-\mu(A\cup B -C) \hspace{5mm}\text{by inclusion-exclusion}\\
            &=(\mu(A\cup C)-\mu(C))+(\mu(B\cup C)-\mu(C))-(\mu(A\cup B\cup C)-\mu(C))\\
            &\text{by additivity of measure}\\            &=\mu(A\cup C)+\mu(B\cup C)-\mu(A\cup b\cup C)-\mu(C).
        \end{align*}
    \end{proof}
\end{bbox}
Construction of the $I$-measure $\mu^*$ on $F_n$:
\begin{itemize}
    \item $\mu^*$ is meaningful if it is consistent with all Shannon's information measures via the substitution of symbols: for all subsets $G, G', G''$ of $N_n$\begin{equation*}
        \mu^*(\hat{X_G}\cap \hat{X_{G'}}-\hat{X_G''})=\I(X_G;X_{G'}|X_{G''})
    \end{equation*}
    Special Cases:\begin{itemize}
        \item $G''=\emptyset$:
        \begin{equation*}
            \mu^*(\hat{X_G}\cap \hat{X_G'}=I(X_G;X_G')
        \end{equation*}
        \item 
        $G=G'$:\begin{equation*}
            \mu*(\hat{X_G}-\hat{X_G''}=H(X_G|X_G'')
        \end{equation*}
        \item $G=G''$ and $G''=\empty$, $\mu*$ is consistent with entropy.
    \end{itemize}
\end{itemize}
\begin{bbox}{3.9 Uniqueness of $\mu^*$}
    $\mu^*$ is the unique signed measure on $F_N$ which is consistent with all Shannon's information measures.
\end{bbox}
\subsection{\texorpdfstring{$\mu^*$}{\mu} can be negative}
For $n=2$, $\mu^*$ is always non-negative. This is because the values of $\mu^*$ on the non-empty atoms of $F_2$ are all Shannon's information measures\\
For $n=3$, $mu^*(\hat{X_1}\cap\hat{X_2}\cap\hat{X_3}) = I(X_1;X_2;X_3)$ can be negative (note this is not a Shannon's information measure). There are 7 non-empty atoms. One of the atoms corresponds to $I(X_1;X_2;X_3)$, which does not correspond to any information measure.
\begin{pbox}{Ex 3.10 $\mu^*$ can be negative}
    Let $X_1$ and $X_2$ be independent binary random variables with uniform distributions: $P(X_i=0)=P(X_i=1)=0.5$. Let $X_3 = (X_1+X_2)\mod 2$ (check whether $X_1$ and $X_2$ are different). \\
    Then $H(X_i) = 1$ for $i=1,2,3$.\\
    Also, the 3 RVs
    are pairwise independent.
    Therefore, \begin{equation*}
        H(X_i;X_j)=H(X_i)+H(X_j)=1+1=2
    \end{equation*} and \begin{equation*}
        I(X_i;X_j)=0.
    \end{equation*}
$H(X_3|X_1;X_2)=0$.
Then by chain rule,\begin{align*}
    H(X_1,X_2,X_3)&=H(X_1,X_2)+(X_3|X_1,X_2)\\
    &= 2+0 = 2
\end{align*}
Now \begin{align*}
    I(X_i;X_j|X_k)&=H(X_i,X_k)+H(X_j,X_k)-H(X_i,X_j,X_k)-H(X_k)\\
    &=2+2-2-1=1
\end{align*}
Then \begin{align*}
    &\mu^*(\Tilde{X_1}\cap\Tilde{X_2}\cap\Tilde{X_3})\\
    &= \mu^*(\Tilde{X_1}\cap\Tilde{X_2})-\mu^*(\hat{X_1}\cap\hat{X_2}-\hat{X_3})\\
    &=0-1=-1 <0
\end{align*}
 \end{pbox}
\end{document}
