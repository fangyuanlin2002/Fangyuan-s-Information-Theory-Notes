\documentclass[../main.tex]{subfiles}
\begin{document}
\section{Achievability of Information Rate-Distortion $R_I(D)$}
Recall: \begin{bbox}{The Rate-Distoriton Theorem}
    \[
    R(D) = R_I(D)
    \]
\end{bbox}
How to prove achievability:
\begin{itemize}
    \item An iid source $\{X_k\}$ with generic random variable $X\sim p(x)$ is given.
    \item For every random variable $\hat X$ taking values in $\hat \X$ with $\bE d(X,\hat X)\leq D$, where $0\leq D\leq D_{max},$ prove that the rate-distortion pair $(I(X;\hat X),D)$ is achievable by showing for large $n$, there exists a rate-distortion code such that \begin{enumerate}
        \item The rate of the code is not more than $I(X;\hat X)+\epsilon$
        \item $d(X,\X) \leq D+\epsilon$ with probability close to $1.$
    \end{enumerate}
    \item Then minimize $I(X;\hat X)$ over all such $\hat X$ to conclude that $(R_I(D),D)$ is achievable.
    \item This implies that $R_I(D)\geq R(D)$
\end{itemize}
We use the random coding scheme technique like before, which uses the properties of joint typicality.
\centering{Random Coding Scheme}
Parameter Settings
\begin{enumerate}
    \item Fix $\epsilon > 0$ and $\hat X$ with $\bE(X,\hat X)\leq D$, where $0\leq D\leq D_{max}.$ Let $\delta$ be specified later.
    \item Let $M$ be an integer satisfying \[
    I(X;\hat X)+\frac{\epsilon}{2} \leq \frac{1}{n}\log M\leq I(X;\hat X) +\epsilon
    \] where $n$ is sufficiently large for such $M$ to exist.
\end{enumerate}
The Random Coding Scheme
\begin{enumerate}
    \item Construct a codebook $\C$ of an $(n,M)$ code by randomly generating $M$ codewords in $\X^n$ independently and identically according to $p(\hat x)^n.$ Denote these codewords by $\hat X(1),\dots,\hat X(M)$
    \item Reveal the codebook $\C$ to both the encoder and the decoder.
    \item The source sequence $\vec X$ is generated according to $p(x)^n.$
    \item The encoder encodes the source sequence $\vec X$ into an index $K$ in the set $\I=\{1,2,\dots, M\}.$ The index $K$ takes the value $i$ if \begin{enumerate}
        \item $(\vec X, \vec{\hat X}(i))\in T^n_{[X\hat X]_\delta}$
        \item for all $i'\in \I,$ if $(\vec X, \vec{\hat X}(i'))\in T^n_{[X\hat X]_\delta},$ then $i'\leq i.$ 
        \newline
        That is, if there exists more than one $i$ satisfying the first condition, let $K$ be the largest one. Otherwise $K$ takes the constant value $1.$ (Give up encoding it.)
    \end{enumerate}
    \item The index $K$ is delivered to the decoder.
    \item The decoder outputs $\vec{\hat X}(K)$ as the reproduction sequence.
 \end{enumerate}
 \begin{remark}
     \begin{itemize}
         \item The event $\{K=1\}$ occurs in one of the following two scenarios: \begin{enumerate}
             \item $\hat X(1)$ is the only codeword in $\C$ which is jointly typical with $\vec X$.
             \item No codeword in $\C$ is jointly typical with $\vec X$.
         \end{enumerate}
         \item If $K\neq 1$, then $\vec{\hat X}(K)$ is always jointly typical with $\vec X$.
     \end{itemize}
 \end{remark}
 \centering{Performance Analysis}
 \begin{enumerate}
     \item As remarked above, the event $\{K=1\}$ occurs in one of the following two scenarios: \begin{enumerate}
             \item $\hat X(1)$ is the only codeword in $\C$ which is jointly typical with $\vec X$.
             \item No codeword in $\C$ is jointly typical with $\vec X$.
         \end{enumerate}
    So if $K=1$, then $\vec X$ is jointly typical with none of the codewords $\vec X(2), \vec X(3),\dots,\vec X(M)$. We will show that $\Pr\{K=1\}$ can be made arbitrarily small.
    \item Define the event 
    \[
    E_i = \{(\vec X, \vec{\hat X}(i)\in T^n_{[X\hat X]_\delta})\}
    \]
    
    \item Then \[\{K=1\}\subset E_2^C\cap E_3^C\cap \dots \cap E_M^C
    \]
    \item Since the codewords are generated iid, conditioning on $\{\vec X=x\}$ for any $\vec x\in \X^n$, the events $E_i$ are mutually independent and have the same probability.
    \item Then for any $\vec x\in \X^n,$
    \begin{align*}
    \Pr\{K=1|\vec X=\vec x\} &\leq \Pr\{E_2^C\cap E_3^C\cap \dots \cap E_M^C|\vec X=\vec x\}\\
    &= \prod_{i=2}^M \Pr\{E_i^C|\vec X=\vec x\}\\
    &=(1-\Pr\{E_1|\vec X=\vec x\})^{M-1}
   \end{align*}
   \item We will focus on $\vec x \in S^n_{[X]_\delta}$ where \[
   S^n_{[X]_\delta} = \{\vec x\in T^n_{[X]_\delta}:|T^n_{[\hat X|X]_\delta}|\geq 1\}
   \] because the probability $\Pr\{\vec X\in S^n_{[X]_\delta}\}$ is close to $1$ for large $n$.
   \item For $\vec x\in S^n_{[X]_\delta}$, obtain the following lower bound on $\Pr\{E_1|\vec X=\vec x\}$:\begin{align*}
       P(E_1|\vec X=\vec x) &= P\{(\vec x, \vec{\hat X}(1)) \in T^n_{X\hat X_\delta}\}\\
       &= \sum_{\hat x\in T^n_{[\hat X|X]_\delta}}p(\hat x)\\
       &\geq \sum_{\hat x\in T^n_{[\hat X|X]_\delta}} 2^{-n(H(\hat X)+\eta)}\\
       &\geq 2^{n(H(\vec X|X)-\xi)} \times 2^{-n(H(\hat X)+\eta)}\\
       &= 2^{-nI(X;\hat X)+\zeta}
   \end{align*} where $\zeta = \xi + \eta \to 0$ as $n\to \infty$ and $\delta \to 0$
   \item Therefore, 
   \begin{align*}
       &\Pr\{K=1|\vec X=\vec x\} \\
       &\leq (2^{-nI(X;\hat X)+\zeta})^{M-1}
   \end{align*}
   \item Now note that \[
   \frac{1}{n}\log M\geq I(X;\hat X)+\frac{\epsilon}{2}\iff M\geq 2^{n(I(X;\hat X)+\epsilon/2)}
   \]
   Then \begin{align*}
       &\ln \Pr\{K=1|\vec X=\vec x\}\\
       &\leq (M-1)\ln [1-2^{-n(I(X;\hat X)+\zeta)}]\\
       &\leq -(2^{n(I(X;\hat X)+\frac{\epsilon}{2})}-1)2^{-n(I(X;\hat X)+\zeta)} \quad\text{by the fundamental inequality}
        \\
        &\to -\infty \quad \text{as $n\to\infty$}
\end{align*}
 if we let $\delta$ be sufficiently small so that \[
 \frac{\epsilon}{2}-\zeta > 0.
 \]
 Therefore 
 \[
 \Pr\{K=1|\vec X=\vec x\}\to 0
 \] as $n\to \infty.$
 \item Then for $\vec x\in S$, for sufficiently large $n$, \[
 \Pr\{K=1|\vec X=\vec x\}\leq \frac{\epsilon}{2}
 \]
 \item Lastly,
 \begin{align*}
     &\Pr\{K=1\}\\
     &=\sum_{\vec x\in S}P(K=1|\vec X=\vec x)\Pr(\vec X=\vec x) + \sum_{x\notin S}\Pr(K=1|\vec X=\vec x) \Pr(\vec X=\vec x)\\
     &\leq \sum_{\vec x\in S}\frac{\epsilon}{2}\Pr(\vec X=\vec x) + \sum_{x\notin S}1\times \Pr(\vec X=x)\\
     &\leq \frac{\epsilon}{2}\times 1 + \delta
 \end{align*}
 \item Arrange that $\Pr(K=1) < \epsilon.$
 \end{enumerate}
\centering{Proof Outline}
\begin{enumerate}
    \item Randomly generate $M$ codewords in $\hat \X^n$ according to the product measure $p(\hat x)^n$, where $n$ is large.
    \item $\vec X\in S^n_{[X]_\delta}$ with high probability.
    \item For $\vec x\in S^n_{[X]_\delta}$, by conditional strong AEP, \[
    \Pr\{(\vec X, \vec{\hat X}(i))\in T^n_{[X\hat X]_\delta}|\vec X=\vec x\}\approx 2^{-n\I(X;\hat X)}.
    \]
    \item If $M$ grows with $n$ at a rate higher than $\I(X;\hat X)$, then the probability that there exists at least one $\vec{\hat X}(i)$ which is jointly typical with the source sequence $\vec X$ with respect to $p(x,\hat x)$ is high.
    \item Such an $\vec{\hat X}(i)$, if exists, would have $d(\vec X, \vec{\hat X}) \approx \bE d(X,\hat X)\leq D$, because the joint relative frequency of $(\vec x, \vec{\hat X}(i))\approx p(x,\hat x)$. i.e. if the source sequence and a codeword are jointly typical, then the distortion of the codeword is approximately equal to the distortion between $X$ and $\hat X$. See the proposition below.
    \item Lastly, use this $\vec{\hat X}(i)$ to represent $\vec X$ to satisfy the distortion constraint.
\end{enumerate}
\begin{bbox}{Proposition}
    For $\hat X$ such that $\bE d(X,\hat X)\leq D$, if $(\vec x, \vec{\hat x})\in T^n_{X\hat X} \delta$, then \[
    d(\vec x,\vec{\hat x})\leq D+d_{max}\delta
    \]
    \begin{proof}
        For $(\vec x, \vec{\hat x})$ that are jointly typical, consider \[
        d(\vec x, \vec{\hat x})=\frac{1}{n}\sum_{k=1}^n d(x_k,\hat x_k)
        \]
        This summation can be written as \[
        \frac{1}{n}\sum_{x,\hat x}^n d(x,\hat x) N(x,\hat x|\vec x,\vec{\hat x}) \quad \text{just a notation for counting the occurrences}
        \]
        \[
        =\frac{1}{n}\sum_{x,\hat x}^n d(x,\hat x)(np(x,\hat x)N(x,\hat x|\vec x,\vec{\hat x})-np(x,\hat x))
        \]
        Then we distribute the terms.
        \[
        = \sum_{x,\hat x}^n d(x,\hat x)p(x,\hat x + \sum_{x,\hat x}d(x,\hat x)(\frac{1}{n}N(x,\hat x|\vec x,\vec{\hat x})-p(x,\hat x))
        \]
        \begin{align*}
            &=\bE d(X,\hat X)+\sum_{x,\hat x}d(x,\hat x)(\frac{1}{n}N(x,\hat x|\vec x, \vec{\hat x}) - p(x,\hat x))\\
            &\leq \bE d(X,\hat X)+\sum_{x,\hat x}d(x,\hat x)|\frac{1}{n}N(x,\hat x|\vec x, \vec{\hat x}) - p(x,\hat x)|\\
            &\leq \bE d(X,\hat X)+d_{max}\sum_{x,\hat x}|\frac{1}{n}N(x,\hat x|\vec x, \vec{\hat x}) - p(x,\hat x)|\\
            &\leq \bE d(X,\hat X) + d_{max}\delta \quad \text{by definition of strong typicality}\\
            &\leq D+d_{max}\delta \quad \text{by assumption}
        \end{align*}
    \end{proof}
\end{bbox}
To finish up the proof of achievability, we upper bound $\Pr\{d(\vec X,\vec{\hat X})>D+\epsilon\}$ by conditioning on whether $K$ is $1$. 
\newline
After showing that for any $\hat X$ such that $\bE d(X,\hat X)\leq D$, $(I(X;\hat X),D)$ is achievable. Finally, we minimize $I(X;\hat X)$ over all $\hat X$ with acceptable distortion to conclude that $(R_I(D),D)$ is achievable, i.e., \[
R_I{D}\geq R(D)
\] by definition of $R(D)$.
\begin{pbox}{Quasi-uniform array interpretation}
    We have $\approx 2^{nH(X)}$ sequences in $T^n_{[X]_\delta}$ and $\approx 2^{nI(X;\hat X)}$ codewords in $T^n_{[\hat X]_\delta}$. 
    \newline
    \begin{itemize}
        \item For each codeword that is a typical $\hat X$ sequence, it's jointly typical with approximately $2^{nH(X|\hat X)}$ typical $X$ sequences.
        \item Therefore, the number of codewords must be at least\[
        \frac{2^{nH(X)}}{2^{nH(X|\hat X)}}\approx 2^{nI(X,\hat X)}\geq 2^{nR_I(D)}
        \]
    \end{itemize}
\end{pbox}

\chapter{The Blahut-Arimoto Algorithms}
We will now discuss how to evaluate channel capacity and rate distortion function numerically.
\section{Single-Letter Characterization}
\begin{itemize}
    \item For a DMC $p(y|x)$, the capacity \[
    C=\max_{r(x)}\I(X;Y)
    \] where $r(x)$ is the input distribution, gives the maximum asymptotically achievable rate for reliable coomunication as blocklength $n\to \infty$
    \item This characterization of $C$, in the form of an optimization problem, is called a \textbf{single-letter characterization} because it involves only $p(y|x)$ but not $n$.
    \item Similarly, the rate-distortion function\[
    R(D) = \min_{Q(\hat x|x):\bE d(X,\hat X)\leq D} I(X;\hat X)
    \] for an i.i.d. information source $\{X_k\}$ is a single-letter characterization because it does not involve the blocklength $n$.
\end{itemize}
\section{Numerical Methods}
\begin{itemize}
    \item When the alphabets are finite, $C$ and $R(D)$ are given as solutions of finite-dimensional optimization problems.
    \item In general, we are not able to express these quantities in closed-forms. In fact, we can only do it in very special cases.
    \item The Blahut-Arimoto algorithms are iterative algorithms devised for this purpose.
\end{itemize}
\subsection{A double supremum}
    Consider the double supremum \[
    \sup_{u_1\in A_1}\sup_{u_2\in A_2}f(u_1,u_2)
    \]
    \begin{itemize}
        \item $A_i$ is a convex subset of $R^{n_i}$
        \item $f:A_1\times A_2\to \bR$ is bounded from above, such that \begin{itemize}
            \item f is continuous and has continuous partial derivaties on $A_1\times A_2$/
            \item For all $u_2\in A_2$, there exists a unique $c_1(u_2)\in A_1$ such that \[
            f(c_1(u_2),u_2)=\max_{u_1'\in A_1}f(u_1',u_2)
            \] and for all $u_1\in A_1$, there exists a unique $c_2(u_1)\in A_2$ such that \[
            f(u_1,c_2(u_1))=\max_{u_2'\in A_2}f(u_1,u_2')
            \]
            \end{itemize}
        \item The supremum of $f$ is taken over the Cartesian product $A:= A_1\times A_2$ and we denote $f^*:=\sup_{\vec u\in A}f(\vec u)$
    \end{itemize}
\subsection{An alternating optimization}
Here is a very intuitive iterative optimization algorithm.
\begin{itemize}
    \item Let $\vec u^{(k)} = (u_1^(k),u_2^(k))$ for $k\geq $ be defined as follows:
    \item Let $u_1^{(0)}$ be an arbitrarily chosen vector is $A_1$, and let $u_2^{(0)}= c_2(u_1^{(0)})$.
    \item For $k\geq 1$, $\vec u^{(k)}$ is defined by \[
    u_1^{(k)}=c_1(u_2^{(k-1)})
    \] and \[
    u_2^{(k)}=c_2(u_1^{(k)})
    \]
    Let $f^{(k)}=f(\vec u^{(k)})$. 
    Then \[
    f^{(k)}\geq f^{(k-1)}
    \] because the points are getting "better and better."
\end{itemize}
\begin{itemize}
    \item This algorithm must converge by the monotone-bounded theorem. $f$ is bounded above by assumption.
    \item We will show that $f^{(k)}\to f^*$ if $f$ is concave.
    \item We can solve the minimization problem by replacing $f$ with $-f$ so that double sup becomes double inf.
    \item This alternating optimization algorithm will be used to compute $C$ and $R(D).$
    \item This algorithm can be visualized if we draw a contour plot. We start at some point and we first move horizontally to find an optimum and then move vertically and stop at a new optimum. Then move horizontally again... Hopefully we can reach the top of the mountain (we certainly will if the mountain is concave.)
\end{itemize}
\section{Computing Channel Capacity}
\begin{bbox}{Lemma about conditional distribution}
    Let $r(x)p(y|x)$ be a given joint distribution on $\X\times\Y$ such that $r>0$. Let $q$ be a transition matrix from $\Y\to \X$. Then \[
    \max_{q} \sum_x\sum_y  r(x)p(y|x)\log\frac{q(x|y)}{r(x)} = \sum_x\sum_y r(x)p(y|x)\log\frac{q^*(x|y)}{r(x)}
    \] where the maximization is taken over all $q$ such that \[
    q(x|y)=0 \quad \text{if and only if} \quad p(y|x)=0
    \] and 
    \[
    q^*(x|y)=\frac{r(x)p(y|x)}{\sum_{x'}r(x')p(y|x)} = \quad \text{the conditional distribution $q_{X|Y}$}
    \]
    \begin{itemize}
        \item Intuitively, we think of $r(x)$ as the input distribution and $p(y|x)$ as the rule of transmission.
    \end{itemize}
    \begin{proof}
        Let \[
        w(y)=\sum_{x'}r(x')p(y|x')
        \]
        Then \[
        r(x)p(y|x)=w(y)q^*(x|y)
        \]
        For any reverse transition matrix $q$, consider \begin{align*}
            &\sum_x\sum_y r(x)p(y|x)\log\frac{q^*(x|y)}{r(x)} - \sum_x\sum_y  r(x)p(y|x)\log\frac{q(x|y)}{r(x)}\\
            &=\sum_x\sum_y r(x)p(y|x)\log\frac{q^*(x|y)}{q(x|y)}\\
            &=\sum_y\sum_x w(y)q^*(x|y)\log\frac{q^*(x|y)}{q(x|y)}\\
            &=\sum_y w(y)\sum_x q^*(x|y)\log\frac{q^*(x|y)}{q(x|y)}\\
            &=\sum_y w(y) D(q*||q)\\
            &\leq 0 \quad \text{by non-negativity of KL divergence/relative entropy}
            \end{align*}
    \end{proof}
\end{bbox}
\begin{bbox}{Recharacterization of Channel Capacity}
    For a discrete memoryless channel $p(y|x)$,\[
    C=\sup_{r>0}\max_q \sum_x\sum_y r(x)p(y|x)\log \frac{q(x|y)}{r(x)} = \max_{r\geq 0}I(r,p)
    \]
    Here we write $I(X;Y)$ as $I(r,p)$.
    \begin{proof}
        \begin{enumerate}
            \item We first prove \[
            C=\max_{r\geq 0}I(x,p) = \sup_{r>0} I(r,p)
            \]
        \end{enumerate}
        \item let $r^*$ achieve $C$. If $r^* > 0$, then no problem. If $r^*\geq 0$. Since $I(r,p)$ is continuous with respect to $r$, for any $\epsilon > 0$, $\exists \delta >0$ such that if \[
        \| r-r^*\| < \delta
        \] then \[
        |C - I(r,p)|<\epsilon
        \]
        \item In particular, there exists $\hat r>0$ strictly positive such that $\|\hat r-r\|<\delta$. Visualization: $r^*$ is on the boundary of the probability simplex as there are some zero probability masses in $r^*$. Within $Ball(r^*, \delta)$, we have a strictly positive distribution.
        \item Then \[
        C= \max_{r\geq 0}I(x,p) = \sup_{r>0} I(r,p) \geq I(\hat r,p) > C-\epsilon
        \]
        Let $\epsilon\to 0$, we have that $C=\sup_{r>0}I(r,p)$
    \end{proof}
\end{bbox}
\begin{bbox}{The BA Algorithm for computing $C$}
    \begin{enumerate}
        \item Let \[
    f(r,q) = \sum_x\sum_y r(x)p(y|x)\log \frac{q(x|y)}{r(x)}
    \]
    where $r$ plays the role of $u_1$ and $q$ plays the role of $u_2$ in the double supremum problem. 
    \item Let \[
    A_1 = \{(r(x),x\in \X): r(x)>0 \quad \text{and} \sum_xr(x)=1\}\subset \bR^{|\X|}
    \] and 
    \begin{align*}
    A_2 &= 
        (q(x|y), (x,y)\in \X\times\Y):q(x|y)\geq 0,\\
        &q(x|y)>0 \quad \text{iff}\quad p(y|x)>0, \sum_x q(x|y)=1\forall y\in \Y\}\\
        &\subset \bR^{|\X||\Y|}    
    \end{align*}
    \end{enumerate}
    Note that
    \begin{itemize}
        \item Both $A_1$ and $A_2$ are convex.
        \item $f$ is bounded above
        \item By property of $A_2$, all the probabilites involved in the double summation are positive.
        \item Therefore, $f$ is continuous and has continuous partial derivatives on $A=A_1\times A_2$.
        \item By the above theorem, $f^*=C$. (supremum over $q\in A_2$ is in fact a maximum by the above lemma)
    \end{itemize}
\end{bbox} 
\begin{bbox}{Algorithm details}
    \begin{enumerate}
        \item By the above lemma, for any given $r\in A_1$, the unique $q\in A_2$ that maximizes $f$ is given by \[
        q(x|y)=\frac{r(x)p(y|x)}{\sum_{x'}r(x')p(y|x')}
        \]
        \item Using Lagrange multipliers, we can show that for any given $q\in A_2$, the unique input distribution $r$ that maximizes $f$ is given by \[
        r(x) = \frac{\prod_y q(x|y)p(y|x)}{\sum_{x'}\prod_y q(x'|y)p(y|x')}
        \] where $\prod_y$ is over all $y$ such that $p(y|x)>0$
        \item Let $r^{(0)}$ be arbitrary positive input distribution in $A_1$. Then we can compute $q^{(0)}$
    \end{enumerate}
\end{bbox}
\begin{bbox}{Maximizing f given a fixed $q$}
    
\end{bbox}
\end{document}